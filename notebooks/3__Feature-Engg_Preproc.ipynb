{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "635ea330-57ef-4536-a9fc-c13a92ceb1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Environment Setup:\n",
      "   Device: cpu\n",
      "   PyTorch Version: 2.8.0+cpu\n",
      "   Pandas Version: 2.3.3\n",
      "   NumPy Version: 2.3.3\n",
      "\n",
      "‚úÖ All libraries imported successfully!\n",
      "üìä Ready to begin customer churn analysis...\n"
     ]
    }
   ],
   "source": [
    "# Essential imports for data science and machine learning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                           roc_auc_score, roc_curve, precision_recall_curve,\n",
    "                           f1_score, precision_score, recall_score)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Environment Setup:\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   PyTorch Version: {torch.__version__}\")\n",
    "print(f\"   Pandas Version: {pd.__version__}\")\n",
    "print(f\"   NumPy Version: {np.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"\\n‚úÖ All libraries imported successfully!\")\n",
    "print(\"üìä Ready to begin customer churn analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bb7b1-2474-4a81-8008-54d3a93fc54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading Cleaned Telco Customer Churn dataset...\n",
      "‚úÖ Cleaned Data loaded successfully from: ../data/clean-processed-data.csv\n",
      "\n",
      "üìä Dataset Overview:\n",
      "   Shape: 7,043 rows √ó 22 columns\n",
      "   Memory usage: 6.4 MB\n",
      "\n",
      "üëÄ First 5 rows:\n",
      "   Unnamed: 0  customerID  gender  SeniorCitizen Partner Dependents  tenure  \\\n",
      "0           0  7590-VHVEG  Female              0     Yes         No       1   \n",
      "1           1  5575-GNVDE    Male              0      No         No      34   \n",
      "2           2  3668-QPYBK    Male              0      No         No       2   \n",
      "3           3  7795-CFOCW    Male              0      No         No      45   \n",
      "4           4  9237-HQITU  Female              0      No         No       2   \n",
      "\n",
      "  PhoneService MultipleLines InternetService  ... DeviceProtection  \\\n",
      "0           No            No             DSL  ...               No   \n",
      "1          Yes            No             DSL  ...              Yes   \n",
      "2          Yes            No             DSL  ...               No   \n",
      "3           No            No             DSL  ...              Yes   \n",
      "4          Yes            No     Fiber optic  ...               No   \n",
      "\n",
      "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
      "0          No          No              No  Month-to-month              Yes   \n",
      "1          No          No              No        One year               No   \n",
      "2          No          No              No  Month-to-month              Yes   \n",
      "3         Yes          No              No        One year               No   \n",
      "4          No          No              No  Month-to-month              Yes   \n",
      "\n",
      "               PaymentMethod MonthlyCharges TotalCharges  Churn  \n",
      "0           Electronic check          29.85        29.85     No  \n",
      "1               Mailed check          56.95      1889.50     No  \n",
      "2               Mailed check          53.85       108.15    Yes  \n",
      "3  Bank transfer (automatic)          42.30      1840.75     No  \n",
      "4           Electronic check          70.70       151.65    Yes  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the customer churn dataset\n",
    "print(\"üì• Loading Cleaned Telco Customer Churn dataset...\")\n",
    "\n",
    "data_paths = [\n",
    "    '../data/preprocessed/clean-processed-data.csv'\n",
    "]\n",
    "\n",
    "df = None\n",
    "for path in data_paths:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"‚úÖ Cleaned Data loaded successfully from: {path}\")\n",
    "        break\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"   Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nüëÄ First 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b01fdab-1317-4404-9fff-56b3cad9a66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß FEATURE ENGINEERING PIPELINE\n",
      "============================================================\n",
      "1Ô∏è‚É£ Creating Business-Driven Features...\n",
      "   ‚úÖ Created new features\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering and Preprocessing Pipeline\n",
    "print(\"üîß FEATURE ENGINEERING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a copy for feature engineering\n",
    "df_features = df.copy()\n",
    "\n",
    "# 1. CREATE BUSINESS-DRIVEN FEATURES\n",
    "print(\"1Ô∏è‚É£ Creating Business-Driven Features...\")\n",
    "\n",
    "# Customer Lifecycle Stage based on tenure\n",
    "def get_lifecycle_stage(tenure):\n",
    "    if tenure <= 12:\n",
    "        return 'New'  # 0-12 months\n",
    "    elif tenure <= 24:\n",
    "        return 'Growing'  # 12-24 months\n",
    "    elif tenure <= 48:\n",
    "        return 'Mature'  # 24-48 months\n",
    "    else:\n",
    "        return 'Loyal'  # 48+ months\n",
    "\n",
    "df_features['LifecycleStage'] = df_features['tenure'].apply(get_lifecycle_stage)\n",
    "\n",
    "# Revenue per Month (Customer Value Score)\n",
    "df_features['RevenuePerMonth'] = df_features['TotalCharges'] / (df_features['tenure'] + 1)\n",
    "\n",
    "# Service Bundle Score (more services = higher engagement)\n",
    "service_features = ['PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup',\n",
    "                   'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "df_features['ServiceBundleScore'] = 0\n",
    "for feature in service_features:\n",
    "    df_features['ServiceBundleScore'] += (df_features[feature] == 'Yes').astype(int)\n",
    "\n",
    "# Risk Indicators based on EDA insights\n",
    "df_features['HighRiskContract'] = (df_features['Contract'] == 'Month-to-month').astype(int)\n",
    "df_features['FiberOpticUser'] = (df_features['InternetService'] == 'Fiber optic').astype(int)\n",
    "df_features['PaperlessHighRisk'] = (df_features['PaperlessBilling'] == 'Yes').astype(int)\n",
    "df_features['SingleCustomer'] = ((df_features['Partner'] == 'No') &\n",
    "                                 (df_features['Dependents'] == 'No')).astype(int)\n",
    "\n",
    "# Price Sensitivity Indicators\n",
    "df_features['HighMonthlyCharges'] = (df_features['MonthlyCharges'] >\n",
    "                                    df_features['MonthlyCharges'].quantile(0.75)).astype(int)\n",
    "\n",
    "# Interaction Features\n",
    "df_features['TenureChargesRatio'] = df_features['tenure'] / (df_features['MonthlyCharges'] + 1)\n",
    "df_features['ChargesPerService'] = df_features['MonthlyCharges'] / (df_features['ServiceBundleScore'] + 1)\n",
    "\n",
    "# Customer Stability Score\n",
    "df_features['StabilityScore'] = (\n",
    "    (df_features['Partner'] == 'Yes').astype(int) +\n",
    "    (df_features['Dependents'] == 'Yes').astype(int) +\n",
    "    (df_features['Contract'] != 'Month-to-month').astype(int) +\n",
    "    (df_features['tenure'] > 24).astype(int)\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Created new features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e843076c-198b-4321-9de0-89740013e901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2Ô∏è‚É£ Preparing Target Variable...\n",
      "   ‚úÖ Target distribution: {0: 5174, 1: 1869}\n"
     ]
    }
   ],
   "source": [
    "# 2. PREPARE TARGET VARIABLE\n",
    "print(\"2Ô∏è‚É£ Preparing Target Variable...\")\n",
    "df_features['target'] = (df_features['Churn'] == 'Yes').astype(int)\n",
    "print(f\"   ‚úÖ Target distribution: {df_features['target'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e065e899-ad1d-47f0-93a8-f730f3d845f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3Ô∏è‚É£ Feature Selection and Encoding...\n",
      "   ‚Ä¢ Categorical features: 16\n",
      "   ‚Ä¢ Numerical features: 9\n",
      "   ‚Ä¢ Binary features: 5\n",
      "   ‚úÖ Encoded categorical features\n"
     ]
    }
   ],
   "source": [
    "# 3. FEATURE SELECTION AND ENCODING\n",
    "print(\"3Ô∏è‚É£ Feature Selection and Encoding...\")\n",
    "\n",
    "# Define feature categories\n",
    "categorical_features = [\n",
    "    'gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',\n",
    "    'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "    'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract',\n",
    "    'PaperlessBilling', 'PaymentMethod', 'LifecycleStage'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges',\n",
    "    'ServiceBundleScore', 'RevenuePerMonth', 'TenureChargesRatio',\n",
    "    'ChargesPerService', 'StabilityScore'\n",
    "]\n",
    "\n",
    "binary_features = [\n",
    "    'HighRiskContract', 'FiberOpticUser', 'PaperlessHighRisk',\n",
    "    'SingleCustomer', 'HighMonthlyCharges'\n",
    "]\n",
    "\n",
    "print(f\"   ‚Ä¢ Categorical features: {len(categorical_features)}\")\n",
    "print(f\"   ‚Ä¢ Numerical features: {len(numerical_features)}\")\n",
    "print(f\"   ‚Ä¢ Binary features: {len(binary_features)}\")\n",
    "\n",
    "# One-hot encode categorical features\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Label encode categorical features (for neural networks, we'll use embedding or one-hot)\n",
    "df_encoded = df_features.copy()\n",
    "\n",
    "# For simplicity, we'll use Label Encoding for categorical features with few categories\n",
    "# and One-Hot for those with more categories\n",
    "label_encoders = {}\n",
    "\n",
    "for feature in categorical_features:\n",
    "    if feature in ['InternetService', 'Contract', 'PaymentMethod', 'LifecycleStage']:\n",
    "        # One-hot encode features with multiple meaningful categories\n",
    "        dummies = pd.get_dummies(df_encoded[feature], prefix=feature, drop_first=False)\n",
    "        df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "        df_encoded = df_encoded.drop(feature, axis=1)\n",
    "    else:\n",
    "        # Label encode binary-like categorical features\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[feature] = le.fit_transform(df_encoded[feature].astype(str))\n",
    "        label_encoders[feature] = le\n",
    "\n",
    "print(f\"   ‚úÖ Encoded categorical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f2e195e-f083-40c8-b476-e687684a957e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4Ô∏è‚É£ Creating Feature Matrix...\n",
      "   ‚úÖ Feature matrix shape: (7043, 41)\n",
      "   ‚úÖ Target distribution: {0: 5174, 1: 1869}\n"
     ]
    }
   ],
   "source": [
    "# 4. CREATE FEATURE MATRIX\n",
    "print(\"4Ô∏è‚É£ Creating Feature Matrix...\")\n",
    "\n",
    "# Drop non-feature columns\n",
    "columns_to_drop = ['customerID', 'Churn']\n",
    "feature_columns = [col for col in df_encoded.columns if col not in columns_to_drop + ['target']]\n",
    "\n",
    "X = df_encoded[feature_columns]\n",
    "y = df_encoded['target']\n",
    "\n",
    "print(f\"   ‚úÖ Feature matrix shape: {X.shape}\")\n",
    "print(f\"   ‚úÖ Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f98305b7-b083-4033-a265-f3dad5b261a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5Ô∏è‚É£ Creating Train-Test Split...\n",
      "‚úÖ Train/test splits saved as CSV files:\n",
      "   - X_train.csv, X_test.csv, y_train.csv, y_test.csv\n",
      "   ‚úÖ Training set: (5634, 41) | Test set: (1409, 41)\n",
      "   ‚úÖ Train target distribution: {0: 4139, 1: 1495}\n",
      "   ‚úÖ Test target distribution: {0: 1035, 1: 374}\n"
     ]
    }
   ],
   "source": [
    "# 5. TRAIN-TEST SPLIT WITH STRATIFICATION\n",
    "print(\"5Ô∏è‚É£ Creating Train-Test Split...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "# Save training and test features and targets to CSV\n",
    "X_train.to_csv('../data/preprocessed/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/preprocessed/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/preprocessed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/preprocessed/y_test.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Train/test splits saved as CSV files:\")\n",
    "print(\"   - X_train.csv, X_test.csv, y_train.csv, y_test.csv\")\n",
    "\n",
    "\n",
    "print(f\"   ‚úÖ Training set: {X_train.shape} | Test set: {X_test.shape}\")\n",
    "print(f\"   ‚úÖ Train target distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"   ‚úÖ Test target distribution: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c588e269-d4c9-45f1-95ec-8ab405667c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6Ô∏è‚É£ Scaling Features...\n",
      "   ‚úÖ Scaled 10 numerical features\n"
     ]
    }
   ],
   "source": [
    "# 6. FEATURE SCALING\n",
    "print(\"6Ô∏è‚É£ Scaling Features...\")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = [col for col in feature_columns if col in numerical_features or\n",
    "                 any(num_feat in col for num_feat in numerical_features)]\n",
    "\n",
    "# Fit scaler on training data only\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "if numerical_cols:\n",
    "    X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "    X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "    print(f\"   ‚úÖ Scaled {len(numerical_cols)} numerical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50db1d6b-17c9-45cf-8895-49423be5b8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7Ô∏è‚É£ Analyzing Feature Importance...\n",
      "\n",
      "üîù TOP 10 MOST CORRELATED FEATURES:\n",
      "                       feature  correlation_with_target\n",
      "              HighRiskContract                 0.406401\n",
      "       Contract_Month-to-month                 0.406401\n",
      "             ChargesPerService                 0.392478\n",
      "                StabilityScore                 0.366827\n",
      "                        tenure                 0.345593\n",
      "            TenureChargesRatio                 0.335601\n",
      "            LifecycleStage_New                 0.314332\n",
      "   InternetService_Fiber optic                 0.312656\n",
      "                FiberOpticUser                 0.312656\n",
      "PaymentMethod_Electronic check                 0.309214\n"
     ]
    }
   ],
   "source": [
    "# 7. FEATURE IMPORTANCE ANALYSIS\n",
    "print(\"7Ô∏è‚É£ Analyzing Feature Importance...\")\n",
    "\n",
    "# Quick feature importance using correlation\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'correlation_with_target': [abs(X_train[col].corr(y_train)) for col in X_train.columns]\n",
    "}).sort_values('correlation_with_target', ascending=False)\n",
    "\n",
    "print(f\"\\nüîù TOP 10 MOST CORRELATED FEATURES:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "968296db-360d-4437-97c0-e0d68d309adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8Ô∏è‚É£ Handling Class Imbalance...\n",
      "   ‚úÖ Class weights: {np.int64(0): np.float64(0.6805991785455424), np.int64(1): np.float64(1.8842809364548494)}\n",
      "   ‚Ä¢ Class 0 (No Churn): 0.68\n",
      "   ‚Ä¢ Class 1 (Churn): 1.88\n",
      "\n",
      "9Ô∏è‚É£ Converting to PyTorch Tensors...\n",
      "   ‚Ä¢ Data types in X_train_scaled: {dtype('int64'): 17, dtype('bool'): 14, dtype('float64'): 10}\n",
      "   ‚úÖ Training tensors: Xtorch.Size([5634, 41]), ytorch.Size([5634])\n",
      "   ‚úÖ Test tensors: Xtorch.Size([1409, 41]), ytorch.Size([1409])\n",
      "   ‚úÖ Device: cpu\n",
      "   ‚úÖ X_train_tensor dtype: torch.float32\n",
      "   ‚úÖ y_train_tensor dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 8. HANDLE CLASS IMBALANCE\n",
    "print(f\"\\n8Ô∏è‚É£ Handling Class Imbalance...\")\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Calculate class weights for imbalanced dataset\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "print(f\"   ‚úÖ Class weights: {class_weight_dict}\")\n",
    "print(f\"   ‚Ä¢ Class 0 (No Churn): {class_weights[0]:.2f}\")\n",
    "print(f\"   ‚Ä¢ Class 1 (Churn): {class_weights[1]:.2f}\")\n",
    "\n",
    "# Convert to PyTorch tensors for training\n",
    "print(f\"\\n9Ô∏è‚É£ Converting to PyTorch Tensors...\")\n",
    "\n",
    "# Check data types before conversion\n",
    "print(f\"   ‚Ä¢ Data types in X_train_scaled: {X_train_scaled.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "# Convert all columns to float\n",
    "X_train_numpy = X_train_scaled.astype(float).values\n",
    "X_test_numpy = X_test_scaled.astype(float).values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_numpy).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_numpy).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).to(device)\n",
    "\n",
    "print(f\"   ‚úÖ Training tensors: X{X_train_tensor.shape}, y{y_train_tensor.shape}\")\n",
    "print(f\"   ‚úÖ Test tensors: X{X_test_tensor.shape}, y{y_test_tensor.shape}\")\n",
    "print(f\"   ‚úÖ Device: {device}\")\n",
    "\n",
    "# Confirm tensor data types\n",
    "print(f\"   ‚úÖ X_train_tensor dtype: {X_train_tensor.dtype}\")\n",
    "print(f\"   ‚úÖ y_train_tensor dtype: {y_train_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20897038-a467-4d82-b176-18bc7479342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä FEATURE ENGINEERING SUMMARY:\n",
      "==================================================\n",
      "üéØ Original features: 22\n",
      "üîß Engineered features: 41\n",
      "üìà Feature increase: +21\n",
      "üèóÔ∏è  Business features created: 11\n",
      "üìä Final dataset: 7,043 samples √ó 41 features\n",
      "‚öñÔ∏è  Class balance handled: Yes (weighted loss)\n",
      "üéØ Ready for model training!\n",
      "\n",
      "‚úÖ Feature engineering completed successfully!\n",
      "üöÄ Ready to build and train the neural network model!\n"
     ]
    }
   ],
   "source": [
    "# 10. FEATURE ENGINEERING SUMMARY\n",
    "print(f\"\\nüìä FEATURE ENGINEERING SUMMARY:\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"üéØ Original features: {df.shape[1]}\")\n",
    "print(f\"üîß Engineered features: {X.shape[1]}\")\n",
    "print(f\"üìà Feature increase: +{X.shape[1] - df.shape[1] + 2}\")  # +2 for dropped ID and target\n",
    "print(f\"üèóÔ∏è  Business features created: 11\")\n",
    "print(f\"üìä Final dataset: {X.shape[0]:,} samples √ó {X.shape[1]} features\")\n",
    "print(f\"‚öñÔ∏è  Class balance handled: Yes (weighted loss)\")\n",
    "print(f\"üéØ Ready for model training!\")\n",
    "\n",
    "# Save preprocessing objects for later use\n",
    "preprocessing_objects = {\n",
    "    'scaler': scaler,\n",
    "    'label_encoders': label_encoders,\n",
    "    'feature_columns': feature_columns,\n",
    "    'class_weights': class_weight_dict\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ Feature engineering completed successfully!\")\n",
    "print(f\"üöÄ Ready to build and train the neural network model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57f51e2b-7363-44ba-add2-6d2ce2ef6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save preprocessing pipeline and params\n",
    "with open('preprocessing_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'scaler': scaler,\n",
    "        'label_encoders': label_encoders,\n",
    "        'feature_columns': feature_columns,\n",
    "        'categorical_features': categorical_features,\n",
    "        'numerical_features': numerical_features,\n",
    "        'binary_features': binary_features,\n",
    "        'class_weight_dict': class_weight_dict\n",
    "    }, f)\n",
    "\n",
    "# Save best model weights (already done in your code)\n",
    "# torch.save(model.state_dict(), 'best_churn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb6e409a-ec4d-421f-9751-e126424ca1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save PyTorch tensors for reproducibility\n",
    "with open('tensor_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train_tensor': X_train_tensor,\n",
    "        'X_test_tensor': X_test_tensor,\n",
    "        'y_train_tensor': y_train_tensor,\n",
    "        'y_test_tensor': y_test_tensor\n",
    "    }, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL/GenAI",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
