{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3099f02e-60e4-4762-aba5-3699c6908427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Environment Setup:\n",
      "   Device: cpu\n",
      "   PyTorch Version: 2.8.0+cpu\n",
      "   Pandas Version: 2.3.3\n",
      "   NumPy Version: 2.3.3\n",
      "\n",
      "‚úÖ All libraries imported successfully!\n",
      "üìä Ready to begin customer churn analysis...\n"
     ]
    }
   ],
   "source": [
    "# Essential imports for data science and machine learning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                           roc_auc_score, roc_curve, precision_recall_curve,\n",
    "                           f1_score, precision_score, recall_score)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Environment Setup:\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   PyTorch Version: {torch.__version__}\")\n",
    "print(f\"   Pandas Version: {pd.__version__}\")\n",
    "print(f\"   NumPy Version: {np.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"\\n‚úÖ All libraries imported successfully!\")\n",
    "print(\"üìä Ready to begin customer churn analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1917fb16-5a48-4329-95bc-b31489a797f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining Model Architecture to perform serialization\n",
    "\n",
    "class ChurnPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Neural Network for Customer Churn Prediction\n",
    "\n",
    "    Architecture:\n",
    "    - Input: All engineered features\n",
    "    - Hidden layers with batch normalization and dropout\n",
    "    - Output: Binary classification (churn probability)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_sizes=[256, 128, 64], dropout_rate=0.2):\n",
    "        super(ChurnPredictor, self).__init__()\n",
    "\n",
    "        # Build dynamic architecture\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            # Batch normalization\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            # Activation\n",
    "            layers.append(nn.ReLU())\n",
    "            # Dropout\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        # layers.append(nn.Sigmoid())  # Sigmoid will be applied in the loss function (BCEWithLogitsLoss)\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier/Glorot initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6377d08-4863-4456-a80b-3c5729239254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "# 1. Load preprocessing pipeline\n",
    "with open('../outputs/preprocessing_pipeline.pkl', 'rb') as f:\n",
    "    pipeline_objects = pickle.load(f)\n",
    "    \n",
    "# Extract all preprocessing objects\n",
    "scaler = pipeline_objects['scaler']\n",
    "label_encoders = pipeline_objects['label_encoders']\n",
    "feature_columns = pipeline_objects['feature_columns']\n",
    "categorical_features = pipeline_objects['categorical_features']\n",
    "numerical_features = pipeline_objects['numerical_features']\n",
    "binary_features = pipeline_objects['binary_features']\n",
    "class_weight_dict = pipeline_objects['class_weight_dict']\n",
    "\n",
    "# Save preprocessing objects for later use\n",
    "preprocessing_objects = {\n",
    "    'scaler': scaler,\n",
    "    'label_encoders': label_encoders,\n",
    "    'feature_columns': feature_columns,\n",
    "    'class_weights': class_weight_dict\n",
    "}\n",
    "\n",
    "# 2. Load PyTorch tensors\n",
    "with open('../outputs/tensors/tensor_data.pkl', 'rb') as f:\n",
    "    tensor_objects = pickle.load(f)\n",
    "    \n",
    "# Extract all tensors\n",
    "X_train_tensor = tensor_objects['X_train_tensor']\n",
    "X_test_tensor = tensor_objects['X_test_tensor']\n",
    "y_train_tensor = tensor_objects['y_train_tensor']\n",
    "y_test_tensor = tensor_objects['y_test_tensor']\n",
    "\n",
    "# 3. Load trained model weights (if needed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#4. Load Model Result Metdatda\n",
    "with open('../outputs/model_metadata.pkl', 'rb') as f:\n",
    "    metadata_object = pickle.load(f)\n",
    "\n",
    "roc_auc_test = metadata_object['auc_score']\n",
    "test_f1 = metadata_object['f1_score']\n",
    "test_precision = metadata_object['precision']\n",
    "test_recall = metadata_object['recall']\n",
    "optimal_threshold = metadata_object['optimal_threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "200105ae-79ef-4d9a-8976-510998a6fed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Input size: 41 features\n",
      "   ‚úÖ Model created and moved to cpu\n",
      "   ‚úÖ Total parameters: 52,865\n",
      "\n",
      "üìã MODEL ARCHITECTURE:\n",
      "ChurnPredictor(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=41, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "    (12): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get input size from our feature matrix\n",
    "input_size = X_train_tensor.shape[1]\n",
    "print(f\"   ‚úÖ Input size: {input_size} features\")\n",
    "\n",
    "# Create model instance\n",
    "model = ChurnPredictor(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=[256, 128, 64],  # Progressive reduction\n",
    "    dropout_rate=0.2\n",
    ").to(device)\n",
    "\n",
    "print(f\"   ‚úÖ Model created and moved to {device}\")\n",
    "print(f\"   ‚úÖ Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Model summary\n",
    "print(f\"\\nüìã MODEL ARCHITECTURE:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "313c6526-8920-4e5d-a3de-b12fbffe5363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../outputs/best_churn_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef54cfad-42b9-4bd4-8990-bf2530615a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../data/preprocessed/X_train.csv\")\n",
    "y_train = pd.read_csv(\"../data/preprocessed/y_train.csv\")\n",
    "y_test = pd.read_csv(\"../data/preprocessed/y_test.csv\")\n",
    "y_val_split = pd.read_csv(\"../data/val-split/y_val_split.csv\")\n",
    "\n",
    "with open('../outputs/tensors/train_validation_split_tensors.pkl', 'rb') as f:\n",
    "    tensor_objects = pickle.load(f)\n",
    "    \n",
    "# Extract all tensors\n",
    "X_train_split_tensor = tensor_objects['X_train_split_tensor']\n",
    "X_val_split_tensor = tensor_objects['X_val_split_tensor']\n",
    "y_train_split_tensor = tensor_objects['y_train_split_tensor']\n",
    "y_val_split_tensor = tensor_objects['y_val_split_tensor']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fa6b1ff-cc53-4f50-9146-aa011c3d1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Serialization Implementation\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def serialize_model(model, preprocessing_objects, metadata, base_path=\"../outputs/model-artifacts\"):\n",
    "    \"\"\"\n",
    "    Comprehensive model serialization for production deployment\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained PyTorch model\n",
    "        preprocessing_objects (dict): Dictionary of preprocessing components\n",
    "        metadata (dict): Model metadata and performance metrics\n",
    "        base_path (str): Directory to save model artifacts\n",
    "\n",
    "    Returns:\n",
    "        dict: Paths to all serialized components\n",
    "    \"\"\"\n",
    "    # Create model versioning with timestamp and performance\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_version = f\"churn_model_v{timestamp}\"\n",
    "\n",
    "    if metadata.get('auc_score'):\n",
    "        model_version += f\"_auc{metadata['auc_score']:.3f}\"\n",
    "\n",
    "    # Create directory structure\n",
    "    model_dir = os.path.join(base_path, model_version)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Save paths\n",
    "    paths = {\n",
    "        'base_dir': model_dir,\n",
    "        'model': os.path.join(model_dir, \"model.pt\"),\n",
    "        'model_torchscript': os.path.join(model_dir, \"model_optimized.pt\"),\n",
    "        'preprocessing': os.path.join(model_dir, \"preprocessing.pkl\"),\n",
    "        'metadata': os.path.join(model_dir, \"metadata.json\"),\n",
    "        'version': model_version\n",
    "    }\n",
    "\n",
    "    # 1. Serialize model weights\n",
    "    print(f\"üì¶ Serializing model state dictionary...\")\n",
    "    torch.save(model.state_dict(), paths['model'])\n",
    "\n",
    "    # 2. Create TorchScript version for optimized inference\n",
    "    print(f\"üöÄ Creating TorchScript optimized model...\")\n",
    "    try:\n",
    "        # Create example input for tracing\n",
    "        example_input = torch.zeros((1, model.network[0].in_features),\n",
    "                                   dtype=torch.float32).to(next(model.parameters()).device)\n",
    "\n",
    "        # Use tracing to create TorchScript model\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        traced_model = torch.jit.trace(model, example_input)\n",
    "        torch.jit.save(traced_model, paths['model_torchscript'])\n",
    "        print(f\"   ‚úÖ TorchScript model created successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è TorchScript conversion failed: {str(e)}\")\n",
    "        paths['model_torchscript'] = None\n",
    "\n",
    "    # 3. Save preprocessing components\n",
    "    print(f\"üîß Saving preprocessing components...\")\n",
    "    import pickle\n",
    "    with open(paths['preprocessing'], 'wb') as f:\n",
    "        pickle.dump(preprocessing_objects, f)\n",
    "\n",
    "    # 4. Save metadata with extended information\n",
    "    print(f\"üìù Saving model metadata...\")\n",
    "\n",
    "    # Add additional metadata\n",
    "    metadata.update({\n",
    "        'serialization_timestamp': timestamp,\n",
    "        'model_version': model_version,\n",
    "        'pytorch_version': torch.__version__,\n",
    "        'input_features': model.network[0].in_features,\n",
    "        'architecture': [str(layer) for layer in model.network],\n",
    "        'device': str(next(model.parameters()).device)\n",
    "    })\n",
    "\n",
    "    # Save as JSON\n",
    "    with open(paths['metadata'], 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Model serialization complete!\")\n",
    "    print(f\"üìÅ Model artifacts saved to {model_dir}\")\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b33d995-bd80-43ac-bbcf-47f3d631ece0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Serializing model state dictionary...\n",
      "üöÄ Creating TorchScript optimized model...\n",
      "   ‚úÖ TorchScript model created successfully\n",
      "üîß Saving preprocessing components...\n",
      "üìù Saving model metadata...\n",
      "‚úÖ Model serialization complete!\n",
      "üìÅ Model artifacts saved to ../outputs/model-artifacts\\churn_model_v20251006_001514_auc0.817\n",
      "\n",
      "üìã Serialized Model Components:\n",
      "   ‚Ä¢ base_dir: ../outputs/model-artifacts\\churn_model_v20251006_001514_auc0.817\n",
      "   ‚Ä¢ model: ../outputs/model-artifacts\\churn_model_v20251006_001514_auc0.817\\model.pt\n",
      "   ‚Ä¢ model_torchscript: ../outputs/model-artifacts\\churn_model_v20251006_001514_auc0.817\\model_optimized.pt\n",
      "   ‚Ä¢ preprocessing: ../outputs/model-artifacts\\churn_model_v20251006_001514_auc0.817\\preprocessing.pkl\n",
      "   ‚Ä¢ metadata: ../outputs/model-artifacts\\churn_model_v20251006_001514_auc0.817\\metadata.json\n",
      "   ‚Ä¢ version: churn_model_v20251006_001514_auc0.817\n",
      "\n",
      "üîÑ Model Loading Demonstration:\n",
      "   üìä Loaded model: churn_model_v20251006_001514_auc0.817\n",
      "   üìà Performance: AUC=0.817, F1=0.608\n",
      "   üéØ Optimal threshold: 0.200\n"
     ]
    }
   ],
   "source": [
    "# Example usage with our trained model\n",
    "model_metadata = {\n",
    "    'training_date': datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    'auc_score': roc_auc_test,\n",
    "    'f1_score': test_f1,\n",
    "    'precision': test_precision,\n",
    "    'recall': test_recall,\n",
    "    'optimal_threshold': optimal_threshold,\n",
    "    'training_epochs': 500, #hard-coded for now\n",
    "    'training_dataset_size': len(X_train),\n",
    "    'class_distribution': {\n",
    "        'churn': int(y_train.sum()),\n",
    "        'no_churn': int(len(y_train) - y_train.sum())\n",
    "    },\n",
    "    'feature_count': X_train.shape[1],\n",
    "    'hyperparameters': {\n",
    "        'hidden_layers': [256, 128, 64],\n",
    "        'dropout_rate': 0.2,\n",
    "        'learning_rate': 0.0001,\n",
    "        'weight_decay': 1e-5\n",
    "    }\n",
    "}\n",
    "\n",
    "# Serialize all model components\n",
    "model_paths = serialize_model(model, preprocessing_objects, model_metadata)\n",
    "\n",
    "# Document the serialized artifacts\n",
    "print(\"\\nüìã Serialized Model Components:\")\n",
    "for key, path in model_paths.items():\n",
    "    if path:\n",
    "        print(f\"   ‚Ä¢ {key}: {path}\")\n",
    "\n",
    "# Demonstrate model loading (partial implementation)\n",
    "print(\"\\nüîÑ Model Loading Demonstration:\")\n",
    "\n",
    "def load_model(model_dir):\n",
    "    \"\"\"Basic model loading implementation\"\"\"\n",
    "    # Load metadata\n",
    "    with open(os.path.join(model_dir, \"metadata.json\"), 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    print(f\"   üìä Loaded model: {metadata['model_version']}\")\n",
    "    print(f\"   üìà Performance: AUC={metadata['auc_score']:.3f}, F1={metadata['f1_score']:.3f}\")\n",
    "    print(f\"   üéØ Optimal threshold: {metadata['optimal_threshold']:.3f}\")\n",
    "\n",
    "    # In production: would continue by loading model weights and preprocessing\n",
    "\n",
    "# Demonstrate loading\n",
    "load_model(model_paths['base_dir'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL/GenAI",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
